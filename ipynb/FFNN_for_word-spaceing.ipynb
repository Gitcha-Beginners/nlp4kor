{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 이 문서는 비전공 초보가 제작하였으므로, 틀린 부분이 있을지도 모릅니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 동영상의 내용과 다른 부분\n",
    "- 가장 마지막 \"check result\"부분에서, 동영상에서 학습된 문장이 사용되어, 새로운 문장으로 테스트하도록 수정.\n",
    "- 학습시간은 GTX 1080으로 수행하여 100만 문장이 약 17시간 소요됩니다. (left_gram=2, right_gram=2, layers=4)\n",
    "    - 동영상을 시연할 때는 GTX 1080Ti를 사용하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 부연 설명\n",
    "- corpus에 존재하지 않은 음절(character)은 OneHotVector.to_vector()의 결과로 모든 값이 0인 벡터가 반환됩니다. \n",
    "    - 즉 characters에 없던 음절(문자)가 입력되는 경우, 모두 붙여쓰기됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 설치 및 설정은 아래 페이지를 참고하여 주세요.\n",
    "- https://github.com/bage79/nlp4kor/blob/master/INSTALL.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 다양한 유틸리티 클래스가 배포되었습니다.\n",
    "- https://github.com/bage79/nlp4kor/tree/master/bage_utils\n",
    "- 차후 강좌에서 계속 사용될 예정이오니, 아래에 import 되는 클래스들은 미리 한번 보시면 이해하시는데, 큰 도움이 되실 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FFNN for 한글 띄어쓰기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"img/word_spacing.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 소스 코드: https://github.com/hunkim/DeepLearningZeroToAll/blob/master/lab-09-2-xor-nn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 한글 문자(음절) 데이터를 입력하기 위해 변환하려면? (text -> vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (방법1) 1d-array one-hot-vector (predefined dictionary)\n",
    "- 자음, 모음, 완성형, 영어/숫자/특수문자 의 모든 문자를 one hot vector 로 표시\n",
    "- vector 종류 수 = 11,316\n",
    "- 미리 vector 변환을 생성해 둘 수 있으나, vector의 크기가 매우 큼. 또한 한자등 다른 문자들은 제외됨."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bage_utils.hangul_util import HangulUtil # 한글처리\n",
    "from bage_utils.num_util import NumUtil # 숫자(int, str) 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('자음:', 'len:', len(HangulUtil.JA_LIST), HangulUtil.JA_LIST)\n",
    "print('모음:', 'len:', len(HangulUtil.MO_LIST), HangulUtil.MO_LIST)\n",
    "print('완성형:', 'len:', len(HangulUtil.WANSUNG_LIST), \n",
    "      HangulUtil.WANSUNG_LIST[:10], '...', HangulUtil.WANSUNG_LIST[-10:])\n",
    "print('전체 한글(자음+모음+완성형)', 'len:', NumUtil.comma_str(len(HangulUtil.HANGUL_LIST)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('전체 한글(자음+모음+완성형) + 영어 + 숫자 + 키보드특수문자:', 'len:', NumUtil.comma_str(len(HangulUtil.CHAR_LIST)))\n",
    "print('len(one-hot-vector):', NumUtil.comma_str(len(HangulUtil.to_one_hot_vector('ㄱ'))))\n",
    "print('ㄱ', HangulUtil.to_one_hot_index('ㄱ'), HangulUtil.to_one_hot_vector('ㄱ'))\n",
    "print('ㅏ', HangulUtil.to_one_hot_index('ㅏ'), HangulUtil.to_one_hot_vector('ㅏ'))\n",
    "print('가', HangulUtil.to_one_hot_index('가'), HangulUtil.to_one_hot_vector('가'))\n",
    "print('힣', HangulUtil.to_one_hot_index('힣'), HangulUtil.to_one_hot_vector('힣'))\n",
    "print('A', HangulUtil.to_one_hot_index('A'), HangulUtil.to_one_hot_vector('A'))\n",
    "print('a', HangulUtil.to_one_hot_index('a'), HangulUtil.to_one_hot_vector('a'))\n",
    "print('0', HangulUtil.to_one_hot_index('0'), HangulUtil.to_one_hot_vector('0'))\n",
    "print('?', HangulUtil.to_one_hot_index('?'), HangulUtil.to_one_hot_vector('?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (방법2) 1d-array non-one-hot-vector\n",
    "- 초성, 중성, 종성, 기타 순서로 3개의 vector를 생성한 후, 1d-array로 concate.\n",
    "- 한글이 아닌 경우에는 중성, 종성 부분은 항상 0.\n",
    "- 경우의 수 = (초성개수*중성개수*종성개수) + 영어개수 + 숫자개수 + 특수문자개수 + ....\n",
    "- 문자 종류별로 범위를 정의해 주어야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('초성:', 'len:', len(HangulUtil.CHO_LIST), HangulUtil.CHO_LIST)\n",
    "print('중성:', 'len:', len(HangulUtil.JUNG_LIST), HangulUtil.JUNG_LIST)\n",
    "print('종성:', 'len:', len(HangulUtil.JONG_LIST), HangulUtil.JONG_LIST)\n",
    "print('영어:', 'len:', len(HangulUtil.ENGLISH_LIST), HangulUtil.ENGLISH_LIST)\n",
    "print('숫자:', 'len:', len(HangulUtil.NUM_LIST), HangulUtil.NUM_LIST)\n",
    "print('특수문자:', 'len:', len(HangulUtil.KEYBOARD_SPECIAL_LIST), HangulUtil.KEYBOARD_SPECIAL_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print('len:', len(HangulUtil.to_cho_jung_jong_vector('?')))\n",
    "print(HangulUtil.to_cho_jung_jong_vector('각'))\n",
    "print(HangulUtil.to_cho_jung_jong_vector('ㄱ'))\n",
    "print(HangulUtil.to_cho_jung_jong_vector('a'))\n",
    "print(HangulUtil.to_cho_jung_jong_vector('1'))\n",
    "print(HangulUtil.to_cho_jung_jong_vector('?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (방법3)  1d-array one-hot-vector (from corpus)\n",
    "- 한자 및 다른 문자들도 처리하고 싶고, 한글의 모든 문자를 항상 사용하는 것이 아님.\n",
    "- 따라서, 적당히 큰 말뭉치(corpus)에서 실제 사용되는 음절을 추출하여 사용.\n",
    "- 이 발표자료의 FFNN에서는 이 방법으로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습용 말뭉치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# 총 문장 수 (dump corpus (ko.wikipedia.org) 문장 단위로 분리. 총 문장: 7,601,655 / 총 문서: 518,062)\n",
    "!ls -lh ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz\n",
    "!zcat ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz | wc -l # Ubuntu\n",
    "# !gzcat ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz | wc -l # OSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!zcat ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz | head # Ubuntu (Broken pipe error on only Jupyter)\n",
    "# !gzcat -cd ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.sentences.gz | head # OSX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls -lh ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters\n",
    "!cat ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters | wc -l \n",
    "!cat ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!grep --color=never 한 ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters\n",
    "!grep --color=never 뷁 ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters\n",
    "!grep --color=never 쀓 ~/workspace/nlp4kor-ko.wikipedia.org/ko.wikipedia.org.characters # not found in corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## character (from corpus) -> Vector \n",
    "x_data -> y_data => 로 표시"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./img/FFNN_for_word-spacing.001.jpeg\">\n",
    "<img src=\"./img/FFNN_for_word-spacing.002.jpeg\">\n",
    "<img src=\"./img/FFNN_for_word-spacing.003.jpeg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 전처리 (text_preprocessing.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from bage_utils.base_util import is_my_pc\n",
    "from bage_utils.datafile_util import DataFileUtil\n",
    "from bage_utils.dataset import DataSet\n",
    "from bage_utils.file_util import FileUtil\n",
    "from bage_utils.hangul_util import HangulUtil\n",
    "from bage_utils.mongodb_util import MongodbUtil\n",
    "from bage_utils.num_util import NumUtil\n",
    "from bage_utils.one_hot_vector import OneHotVector\n",
    "from nlp4kor.config import log, KO_WIKIPEDIA_ORG_SENTENCES_FILE, KO_WIKIPEDIA_ORG_DATA_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 입력데이터(문장) 파일 생성 (Mongodb -> file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences_file = KO_WIKIPEDIA_ORG_SENTENCES_FILE\n",
    "log.info('sentences_file: %s' % sentences_file)\n",
    "if not os.path.exists(sentences_file):\n",
    "    TextPreprocess.dump_corpus(MONGO_URL, db_name='parsed', collection_name='ko.wikipedia.org', sentences_file=sentences_file,\n",
    "                               mongo_query={})  # mongodb -> text file(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 벡터매핑사전(음절) 파일 생성 (문장-> 음절)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "characters_file = os.path.join(KO_WIKIPEDIA_ORG_DATA_DIR, 'ko.wikipedia.org.characters')\n",
    "log.info('characters_file: %s' % characters_file)\n",
    "if not os.path.exists(characters_file):\n",
    "    log.info('collect characters...')\n",
    "    TextPreprocess.collect_characters(sentences_file, characters_file)  # text file -> characters(unique features)\n",
    "    log.info('collect characters OK.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot vector?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "unary_vector = OneHotVector([0])\n",
    "binary_vector = OneHotVector([0, 1])\n",
    "ternary_vector = OneHotVector([0, 1, 2])\n",
    "print('%6s\\t%6s\\t%6s\\t%6s' % ('', 'unary', 'binary', 'ternary'))\n",
    "for i in [0, 1, 2]:\n",
    "    print('%6s\\t%6s\\t%6s\\t%6s' % (i, unary_vector.to_vector(i), binary_vector.to_vector(i), ternary_vector.to_vector(i)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One hot vector for 음절"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "log.info('load characters list...')\n",
    "features_vector = OneHotVector(DataFileUtil.read_list(characters_file))\n",
    "labels_vector = OneHotVector([0, 1])  # 붙여쓰기=0, 띄어쓰기=1\n",
    "log.info('load characters list OK. len: %s' % NumUtil.comma_str(len(features_vector))) # 데이터셋 마다 character 구성과 개수는 다름.\n",
    "\n",
    "print(features_vector, len(features_vector))\n",
    "print(labels_vector, len(labels_vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for c in ['a', 'b', 'ㄱ', 'ㄲ', 'ㄳ', 'ㄴ', '가', '각']:\n",
    "    v = features_vector.to_vector(c) # one hot vector\n",
    "    _c = features_vector.to_value(v) # character for check\n",
    "    i = features_vector.to_index(c) # index number in characters list (0~17380)\n",
    "    print(c, v, _c, i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset (training data, test data, validation data)\n",
    "- 너무 커서 공유하기 힘드네요.. 가장 큰 파일이 약 26GB\n",
    "- 직접 생성하시면 됩니다. ^^ (sentences, characters 파일 이용)\n",
    "\n",
    "##### ko.wikipedia.org.dataset.문장수.left_gram.right_gram.종류.gz\n",
    "- train: text, int 형식 (one-hot-vector로 저장하면 파일이 너무 커짐)\n",
    "- test, validation: one-hot-vector 형식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!ls -lh ~/workspace/nlp4kor-ko.wikipedia.org/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensorflow model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -lh ~/workspace/nlp4kor-ko.wikipedia.org/models/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ls -lh ~/workspace/nlp4kor-ko.wikipedia.org/models/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip. 제 노트북에 GPU가 없어요. GPU 있는 PC에서 개발해야 하나요?\n",
    "- Pycharm Deployment & Remote Interpreter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 각종 설정값 (word_spacing.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from bage_utils.base_util import is_my_pc\n",
    "from bage_utils.datafile_util import DataFileUtil\n",
    "from bage_utils.dataset import DataSet\n",
    "from bage_utils.datasets import DataSets\n",
    "from bage_utils.num_util import NumUtil\n",
    "from bage_utils.one_hot_vector import OneHotVector\n",
    "from bage_utils.watch_util import WatchUtil\n",
    "from nlp4kor.config import log, KO_WIKIPEDIA_ORG_DATA_DIR, KO_WIKIPEDIA_ORG_SENTENCES_FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if len(sys.argv) == 2:\n",
    "    max_sentences = int(sys.argv[1])\n",
    "else:\n",
    "    max_sentences = int('1,000,000'.replace(',', '')) if is_my_pc() else int('1,000,000'.replace(',', ''))  # run 100 or 1M data (학습: 17시간 소요)\n",
    "# max_sentences = 100 if is_my_pc() else FileUtil.count_lines(sentences_file, gzip_format=True) # run 100 or full data (학습시간: 5일 소요)\n",
    "layers = 4\n",
    "model_file = os.path.join(KO_WIKIPEDIA_ORG_DATA_DIR, 'models',\n",
    "                          'word_spacing_model.sentences=%s.layers=%s/model' % (max_sentences, layers))  # .%s' % max_sentences\n",
    "log.info('max_sentences: %s' % max_sentences)\n",
    "log.info('layers: %s' % layers)\n",
    "log.info('model_file: %s' % model_file)\n",
    "\n",
    "sentences_file = KO_WIKIPEDIA_ORG_SENTENCES_FILE\n",
    "log.info('sentences_file: %s' % sentences_file)\n",
    "\n",
    "characters_file = os.path.join(KO_WIKIPEDIA_ORG_DATA_DIR, 'ko.wikipedia.org.characters')\n",
    "log.info('characters_file: %s' % characters_file)\n",
    "\n",
    "batch_size = 1000  # mini batch size\n",
    "left_gram, right_gram = 2, 2\n",
    "ngram = left_gram + right_gram\n",
    "log.info('batch_size: %s' % batch_size)\n",
    "log.info('left_gram: %s, right_gram: %s' % (left_gram, right_gram))\n",
    "log.info('ngram: %s' % ngram)\n",
    "\n",
    "features_vector = OneHotVector(DataFileUtil.read_list(characters_file))\n",
    "labels_vector = OneHotVector([0, 1])  # 붙여쓰기=0, 띄어쓰기=1\n",
    "n_features = len(features_vector) * ngram  # number of features = 17,380 * 4\n",
    "n_classes = len(labels_vector) if len(labels_vector) >= 3 else 1  # number of classes = 2 but len=1\n",
    "log.info('features_vector: %s' % features_vector)\n",
    "log.info('labels_vector: %s' % labels_vector)\n",
    "log.info('n_features: %s' % n_features)\n",
    "log.info('n_classes: %s' % n_classes)\n",
    "\n",
    "n_hidden1 = 100\n",
    "learning_rate = 0.01  # 0.1 ~ 0.001\n",
    "log.info('n_hidden1: %s' % n_hidden1)\n",
    "log.info('learning_rate: %s' % learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test with samples. (word_spacing.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nlp4kor.ws.word_spacing import WordSpacing\n",
    "log.info('sample testing...')\n",
    "test_set = ['예쁜 운동화', '즐거운 동화', '삼풍동 화재']\n",
    "for s in test_set:\n",
    "    features, labels = WordSpacing.sentence2features_labels(s, left_gram=1, right_gram=1)\n",
    "    log.info('in : \"%s\"' % s)\n",
    "    log.info('%s -> %s' % (features, labels))\n",
    "    log.info('out: \"%s\"' % WordSpacing.spacing(s.replace(' ', ''), labels))\n",
    "log.info('sample testing OK.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 입력데이터 생성, 학습, 평가 (word_spacing.py)\n",
    "- WordSpacing.learning() in word_spacing.py\n",
    "- <u>내용이 너무 길어 실제 소스로 설명합니다.</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not os.path.exists(model_file + '.index') or not os.path.exists(model_file + '.meta'):\n",
    "    WordSpacing.learning(sentences_file, batch_size, left_gram, right_gram, model_file, features_vector, labels_vector, n_hidden1=n_hidden1, max_sentences=max_sentences, learning_rate=learning_rate, layers=layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 그리고... \n",
    "- 지금까지 평가한 것은 4음절 단위에 대한 결과입니다. 하지만 우리가 원하는 것은 문장을 입력으로 했을 때의 결과죠.\n",
    "- 새로운 문장으로 제대로 띄어쓰기가 되는지 확인해 봅니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "log.info('chek result...')\n",
    "watch = WatchUtil()\n",
    "watch.start('read sentences')\n",
    "\n",
    "sentences = ['아버지가 방에 들어 가신다.', '가는 말이 고와야 오는 말이 곱다.']\n",
    "max_test_sentences = 100\n",
    "with gzip.open(sentences_file, 'rt') as f:\n",
    "    if max_test_sentences < max_sentences:  # leared sentences is smaller than full sentences\n",
    "        for i, line in enumerate(f, 1):\n",
    "            if i <= max_sentences:  # skip learned sentences\n",
    "                if i % 100000 == 0:\n",
    "                    log.info('skip %d th learned sentence.' % i)\n",
    "                continue\n",
    "            if len(sentences) >= max_test_sentences:  # read new sentences\n",
    "                break\n",
    "\n",
    "            s = line.strip()\n",
    "            if s.count(' ') > 0:  # sentence must have one or more space.\n",
    "                sentences.append(s)\n",
    "log.info('len(sentences): %s' % NumUtil.comma_str(len(sentences)))\n",
    "watch.stop('read sentences')\n",
    "\n",
    "watch.start('run tensorflow')\n",
    "accuracies, sims = [], []\n",
    "with tf.Session() as sess:\n",
    "    graph = WordSpacing.build_FFNN(n_features, n_classes, n_hidden1, learning_rate, layers=layers)\n",
    "    X, Y, predicted, accuracy = graph['X'], graph['Y'], graph['predicted'], graph['accuracy']\n",
    "\n",
    "    saver = tf.train.Saver()\n",
    "    try:\n",
    "        restored = saver.restore(sess, model_file)\n",
    "    except:\n",
    "        log.error('restore failed. model_file: %s' % model_file)\n",
    "    try:\n",
    "        for i, s in enumerate(sentences):\n",
    "            log.info('')\n",
    "            log.info('[%s] in : \"%s\"' % (i, s))\n",
    "            features, labels = WordSpacing.sentence2features_labels(s, left_gram, right_gram)\n",
    "            dataset = DataSet(features=features, labels=labels, features_vector=features_vector, labels_vector=labels_vector)\n",
    "            dataset.convert_to_one_hot_vector()\n",
    "            if len(dataset) > 0:\n",
    "                _predicted, _accuracy = sess.run([predicted, accuracy], feed_dict={X: dataset.features, Y: dataset.labels})  # Accuracy report\n",
    "\n",
    "                generated_sentence = WordSpacing.spacing(s.replace(' ', ''), _predicted)\n",
    "                sim, correct, total = WordSpacing.sim_two_sentence(s, generated_sentence, left_gram=left_gram, right_gram=right_gram)\n",
    "\n",
    "                accuracies.append(_accuracy)\n",
    "                sims.append(sim)\n",
    "\n",
    "                log.info('[%s] out: \"%s\" (accuracy: %.1f%%, sim: %.1f%%=%s/%s)' % (i, generated_sentence, _accuracy * 100, sim * 100, correct, total))\n",
    "    except:\n",
    "        log.error(traceback.format_exc())\n",
    "\n",
    "log.info('chek result OK.')\n",
    "# noinspection PyStringFormat\n",
    "log.info('mean(accuracy): %.2f%%, mean(sim): %.2f%%' % (np.mean(accuracies) * 100, np.mean(sims) * 100))\n",
    "log.info('secs/sentence: %.4f' % (watch.elapsed('run tensorflow') / len(sentences)))\n",
    "log.info(watch.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip. 입력 데이터 증가, 파라미터 조절, 레이어 증가... 성능을 높이려면, 뭐 부터 해야 할까요?\n",
    "- 여러 가지 해보려면 시간이 적게 걸리는 것부터 해야 겠죠.\n",
    "- 파라미터 조절 or 레이어 증가 -> 입력 데이터 증가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
